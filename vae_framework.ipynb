{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vae_framework.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":["QFXcRi1Az-rM","BBj7Wm5y0B4I","mNH42kQi0KRn"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"SbJoUgS4zD2-","colab_type":"text"},"cell_type":"markdown","source":["# Variational Auto Encoder Testing Framework\n","This notebook will attempt to augment CNNs with VAEs. We will train a VAE on the training set and then take the model to produce signals of reduced dimensionality that will be feeded into a CNN.\n","\n","---\n","\n"]},{"metadata":{"id":"St3LfV04zmdH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!kill -9 -1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dKRF9oEUzej_","colab_type":"text"},"cell_type":"markdown","source":["##Initialization\n","Google Drive access, PyTorch, etc."]},{"metadata":{"id":"x7KDY4loy_HJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4}],"base_uri":"https://localhost:8080/","height":107},"outputId":"2a107c43-e69d-4c59-e77f-3af582cd025c","executionInfo":{"status":"ok","timestamp":1521078994237,"user_tz":420,"elapsed":12522,"user":{"displayName":"Nathan Wong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"106327691903472177650"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"6-GNVlDlzpMw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bdbl-ZprzsFD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A_zfy7fqzvhW","colab_type":"text"},"cell_type":"markdown","source":["##Imports"]},{"metadata":{"id":"ABQEOXrhzt-x","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","import h5py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.cuda\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from scipy import stats"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S_uxabVNz9RK","colab_type":"text"},"cell_type":"markdown","source":["##Classes"]},{"metadata":{"id":"QFXcRi1Az-rM","colab_type":"text"},"cell_type":"markdown","source":["###EEG Dataset"]},{"metadata":{"id":"yGRNH_BW0BZC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class EEGDataset(Dataset):\n","  \"\"\"EEG dataset.\"\"\"\n","  \n","  def __init__(self, x, y, transform=None):\n","    \"\"\"\n","    Args:\n","      x (numpy array): Input data of shape \n","                       num_trials x num_electrodes x num_time_bins.\n","      y (numpy array): Output data of shape num_trials x 1.\n","      transform (callable, optional): Optional transform to be applied.\n","    \"\"\"\n","    self.x = x\n","    self.y = y\n","    self.transform = transform\n","    \n","  def __len__(self):\n","    return len(self.x)\n","  \n","  def __getitem__(self, idx):\n","    x_sample = torch.from_numpy(self.x[idx])\n","    y_sample = torch.IntTensor([int(self.y[idx])])\n","    \n","    if self.transform:\n","      pass #FIXME\n","    \n","    return x_sample, y_sample"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BBj7Wm5y0B4I","colab_type":"text"},"cell_type":"markdown","source":["###EEG Minimal Container"]},{"metadata":{"id":"Ol1bCReH0HxM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class EEGMinimalContainer():\n","  \"\"\"EEG container for training and testing datasets.\"\"\"\n","  \n","  def __init__(self, data_dir, train_subject=None, test_subject=None, \n","               remove_eog_channels=True, seed=42):\n","    \"\"\"\n","    Args:\n","      data_dir (string): Path to all A0iT_slice.mat files for i in [1, 9].\n","      train_subject(int): Subject to train on. If None, train on all.\n","      test_subject(int): Subject to test on. If None, train on all except for\n","                         train_subject. Only used if train_subject is not None.\n","    \"\"\"\n","    self.X_train = None\n","    self.y_train = None\n","    self.X_test = None\n","    self.y_test = None\n","    self.train_dataset = None\n","    self.test_dataset = None\n","    np.random.seed(seed)\n","    \n","    if train_subject is None:\n","      # Step 1: Append all of the input and output data together\n","      X = None\n","      y = None\n","      end = np.empty(9)\n","      for i in np.arange(9):\n","        A0iT = h5py.File(data_dir + ('/A0%dT_slice.mat' % (i+1)), 'r')\n","        X_temp = np.copy(A0iT['image'])\n","        y_temp = np.copy(A0iT['type'])\n","        y_temp = y_temp[0,0:X_temp.shape[0]:1]\n","        y_temp = np.asarray(y_temp, dtype=np.int32)\n","        X = X_temp if X is None else np.append(X, X_temp, axis=0)\n","        y = y_temp if y is None else np.append(y, y_temp, axis=0)\n","        end[i] = X_temp.shape[0] if i == 0 else X_temp.shape[0] + end[i-1]\n","      X = np.expand_dims(X, axis=1)\n","      y -= 769\n","      # Step 2: Remove the EOG\n","      if remove_eog_channels:\n","        X = X[:, :, 0:22, :] \n","      # Step 3: Remove NaN trials\n","      remove_list = []\n","      for i in range(len(X)):\n","        if np.isnan(X[i]).any():\n","          remove_list.append(i)\n","      for trial_row in remove_list:\n","        end[end > trial_row] -= 1\n","      X = np.delete(X, remove_list, axis=0)\n","      y = np.delete(y, remove_list, axis=0)\n","      # Normalize DATASET to [0, 1]\n","      Xmax = np.nanmax(X)\n","      Xmin = np.nanmin(X)\n","      X = (X - Xmin) / (Xmax - Xmin)\n","      # Step 4: Generate an train/test split\n","      remove_list = []\n","      self.X_test = {}\n","      self.y_test = {}\n","      self.test_dataset = {}\n","      sloc = 0\n","      for i, eloc in enumerate(end, 1):\n","        t_list = np.random.choice(np.arange(sloc, eloc), 50, replace=False)\n","        t_list = t_list.astype(int)\n","        self.X_test[str(i)] = X[t_list, :, :, :]\n","        self.y_test[str(i)] = y[t_list]\n","        self.test_dataset[str(i)] = EEGDataset(X[t_list, :, :, :], y[t_list])\n","        remove_list = remove_list + t_list.tolist()\n","        sloc = eloc\n","      self.X_train = np.delete(X, remove_list, axis=0)\n","      self.y_train = np.delete(y, remove_list, axis=0)\n","      self.train_dataset = EEGDataset(self.X_train, self.y_train)\n","      \n","      print('EEGContainer X_train: ' + str(self.X_train.shape))\n","      print('EEGContainer y_train: ' + str(self.y_train.shape))\n","      for i in range(1, 10):\n","        print(('EEGContainer X_test%d: ' %i) + str(self.X_test[str(i)].shape))\n","        print(('EEGContainer y_test%d: ' %i) + str(self.y_test[str(i)].shape))\n","    \n","    else:\n","      pass #FIXME"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mNH42kQi0KRn","colab_type":"text"},"cell_type":"markdown","source":["###Convolutional Neural Network"]},{"metadata":{"id":"_6wgOrj9eXq9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","    self.conv1 = nn.Conv2d( 1, 16, (1, 11), stride=(1, 1), padding=0)\n","    self.conv2 = nn.Conv2d(16, 16, (22, 1), stride=(1, 1), padding=0)\n","    self.conv3 = nn.Conv2d( 1, 16, (1, 11), stride=(1, 1), padding=0)\n","    self.conv4 = nn.Conv2d(16, 16, (16, 1), stride=(1, 1), padding=0)\n","    self.conv5 = nn.Conv2d( 1, 16, 3, stride=1, padding=1)\n","    self.conv6 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n","    self.conv7 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n","    self.fc1 = nn.Linear(64 * 2 * 10, 200)\n","    self.fc2 = nn.Linear(200, 100)\n","    self.fc3 = nn.Linear(100, 4)\n","  \n","  def forward(self, x):\n","    dropval = 0.7\n","    x = F.dropout2d(F.relu(self.conv1(x)), p=0.2)\n","    x = F.dropout2d(F.relu(self.conv2(x)), p=dropval)\n","    x = x.permute(0, 2, 1, 3)\n","    x = F.max_pool2d(x, (1, 3), (1, 3))\n","    x = F.dropout2d(F.relu(self.conv3(x)), p=dropval)\n","    x = F.dropout2d(F.relu(self.conv4(x)), p=dropval)\n","    x = x.permute(0, 2, 1, 3)\n","    x = F.max_pool2d(x, (1, 4), (1, 4))\n","    x = F.dropout2d(F.relu(self.conv5(x)), p=dropval)\n","    x = F.max_pool2d(x, 2, 2)\n","    x = F.dropout2d(F.relu(self.conv6(x)), p=dropval)\n","    x = F.max_pool2d(x, 2, 2)\n","    x = F.dropout2d(F.relu(self.conv7(x)), p=dropval)\n","    x = F.max_pool2d(x, 2, 2)\n","    x = x.view(-1, 64 * 2 * 10)\n","    x = F.dropout(F.relu(self.fc1(x)), p=dropval)\n","    x = F.dropout(F.relu(self.fc2(x)), p=dropval)\n","    x = self.fc3(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LnWlm9YV0qhq","colab_type":"text"},"cell_type":"markdown","source":["###Variational Auto Encoder"]},{"metadata":{"id":"XeN-oMKn0yZm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","        \n","        ZDIMS = 20\n","\n","        # ENCODER\n","        # 22 x time_batch input pixels, 400 outputs\n","        self.fc1 = nn.Linear(22000, 400)\n","        # rectified linear unit layer from 400 to 400\n","        # max(0, x)\n","        self.relu = nn.ReLU()\n","        self.fc21 = nn.Linear(400, ZDIMS)  # mu layer\n","        self.fc22 = nn.Linear(400, ZDIMS)  # logvariance layer\n","        # this last layer bottlenecks through ZDIMS connections\n","\n","        # DECODER\n","        # from bottleneck to hidden 400\n","        self.fc3 = nn.Linear(ZDIMS, 400)\n","        # from hidden 400 to 22 x time_batch outputs\n","        self.fc4 = nn.Linear(400, 22000)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def encode(self, x: Variable) -> (Variable, Variable):\n","        \"\"\"Input vector x -> fully connected 1 -> ReLU -> (fully connected\n","        21, fully connected 22)\n","\n","        Parameters\n","        ----------\n","        x : [128, 784] matrix; 128 digits of 28x28 pixels each\n","\n","        Returns\n","        -------\n","\n","        (mu, logvar) : ZDIMS mean units one for each latent dimension, ZDIMS\n","            variance units one for each latent dimension\n","\n","        \"\"\"\n","\n","        # h1 is [128, 400]\n","        h1 = self.relu(self.fc1(x))  # type: Variable\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n","        \"\"\"THE REPARAMETERIZATION IDEA:\n","\n","        For each training sample (we get 128 batched at a time)\n","\n","        - take the current learned mu, stddev for each of the ZDIMS\n","          dimensions and draw a random sample from that distribution\n","        - the whole network is trained so that these randomly drawn\n","          samples decode to output that looks like the input\n","        - which will mean that the std, mu will be learned\n","          *distributions* that correctly encode the inputs\n","        - due to the additional KLD term (see loss_function() below)\n","          the distribution will tend to unit Gaussians\n","\n","        Parameters\n","        ----------\n","        mu : [128, ZDIMS] mean matrix\n","        logvar : [128, ZDIMS] variance matrix\n","\n","        Returns\n","        -------\n","\n","        During training random sample from the learned ZDIMS-dimensional\n","        normal distribution; during inference its mean.\n","\n","        \"\"\"\n","\n","        if self.training:\n","            # multiply log variance with 0.5, then in-place exponent\n","            # yielding the standard deviation\n","            std = logvar.mul(0.5).exp_()  # type: Variable\n","            # - std.data is the [128,ZDIMS] tensor that is wrapped by std\n","            # - so eps is [128,ZDIMS] with all elements drawn from a mean 0\n","            #   and stddev 1 normal distribution that is 128 samples\n","            #   of random ZDIMS-float vectors\n","            eps = Variable(std.data.new(std.size()).normal_())\n","            # - sample from a normal distribution with standard\n","            #   deviation = std and mean = mu by multiplying mean 0\n","            #   stddev 1 sample with desired std and mu, see\n","            #   https://stats.stackexchange.com/a/16338\n","            # - so we have 128 sets (the batch) of random ZDIMS-float\n","            #   vectors sampled from normal distribution with learned\n","            #   std and mu for the current input\n","            return eps.mul(std).add_(mu)\n","\n","        else:\n","            # During inference, we simply spit out the mean of the\n","            # learned distribution for the current input.  We could\n","            # use a random sample from the distribution, but mu of\n","            # course has the highest probability.\n","            return mu\n","\n","    def decode(self, z: Variable) -> Variable:\n","        h3 = self.relu(self.fc3(z))\n","        return self.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n","        mu, logvar = self.encode(x.view(-1, 22000))\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vTadV83KR7o3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def vae_loss_function(recon_x, x, mu, logvar):\n","  \n","  BCE = F.binary_cross_entropy(recon_x, x.view(-1, 22000), size_average=False)\n","\n","  # see Appendix B from VAE paper:\n","  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","  # https://arxiv.org/abs/1312.6114\n","  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","  #print(logvar)\n","  #print(logvar.exp)\n","  KLD = 0#-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","  \n","  return BCE + KLD"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q2eUjKrPOuxN","colab_type":"text"},"cell_type":"markdown","source":["##Setup"]},{"metadata":{"id":"640NoD7zOxpF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["data_dir = 'drive/ee239as/project_datasets'\n","batch_size = 31"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lm2BHG8gO1FI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":369},"outputId":"cef7a7a8-0751-47f3-a9fc-1472799febaa","executionInfo":{"status":"ok","timestamp":1521080378287,"user_tz":420,"elapsed":6887,"user":{"displayName":"Nathan Wong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"106327691903472177650"}}},"cell_type":"code","source":["EEGset = EEGMinimalContainer(data_dir)\n","\n","train_loader = torch.utils.data.DataLoader(EEGset.train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_loader = {}\n","for i in range(1, 10):\n","  test_loader[str(i)] = torch.utils.data.DataLoader(EEGset.test_dataset[str(i)], batch_size=1, shuffle=False)"],"execution_count":72,"outputs":[{"output_type":"stream","text":["EEGContainer X_train: (2108, 1, 22, 1000)\n","EEGContainer y_train: (2108,)\n","EEGContainer X_test1: (50, 1, 22, 1000)\n","EEGContainer y_test1: (50,)\n","EEGContainer X_test2: (50, 1, 22, 1000)\n","EEGContainer y_test2: (50,)\n","EEGContainer X_test3: (50, 1, 22, 1000)\n","EEGContainer y_test3: (50,)\n","EEGContainer X_test4: (50, 1, 22, 1000)\n","EEGContainer y_test4: (50,)\n","EEGContainer X_test5: (50, 1, 22, 1000)\n","EEGContainer y_test5: (50,)\n","EEGContainer X_test6: (50, 1, 22, 1000)\n","EEGContainer y_test6: (50,)\n","EEGContainer X_test7: (50, 1, 22, 1000)\n","EEGContainer y_test7: (50,)\n","EEGContainer X_test8: (50, 1, 22, 1000)\n","EEGContainer y_test8: (50,)\n","EEGContainer X_test9: (50, 1, 22, 1000)\n","EEGContainer y_test9: (50,)\n"],"name":"stdout"}]},{"metadata":{"id":"NVyXpIF6O6q4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["num_epochs = 200\n","learning_rate = 1e-5\n","\n","use_cuda = True\n","\n","net = CNN()\n","model = VAE()\n","criterion = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wcFfTG8HO-HY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["if use_cuda and torch.cuda.is_available():\n","  net.cuda()\n","  model.cuda()\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-3)\n","vae_optimizer = optim.Adam(model.parameters(), lr=5e-3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TTvUjbkCROZv","colab_type":"text"},"cell_type":"markdown","source":["##Training"]},{"metadata":{"id":"-RAOzvVtUHDN","colab_type":"text"},"cell_type":"markdown","source":["###VAE Training Functions"]},{"metadata":{"id":"FbAWmb7QRPjl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def vae_train(epoch):\n","    # toggle model to train mode\n","    model.train()\n","    train_loss = 0\n","    # in the case of MNIST, len(train_loader.dataset) is 60000\n","    # each `data` is of BATCH_SIZE samples and has shape [128, 1, 28, 28]\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = Variable(data.type(torch.FloatTensor))\n","        if use_cuda and torch.cuda.is_available():\n","            data = data.cuda()\n","        vae_optimizer.zero_grad()\n","\n","        # push whole batch of data through VAE.forward() to get recon_loss\n","        recon_batch, mu, logvar = model(data)\n","        # calculate scalar loss\n","        loss = vae_loss_function(recon_batch, data, mu, logvar)\n","        # calculate the gradient of the loss w.r.t. the graph leaves\n","        # i.e. input variables -- by the power of pytorch!\n","        loss.backward()\n","        train_loss += loss.data[0]\n","        vae_optimizer.step()\n","        #if batch_idx % 17 == 0:\n","        #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","        #        epoch, batch_idx * len(data), len(train_loader.dataset),\n","        #        100. * batch_idx / len(train_loader),\n","        #        loss.data[0] / len(data)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","          epoch, train_loss / len(train_loader.dataset)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VipNGIX6S8u6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def vae_test(epoch):\n","    # toggle model to test / inference mode\n","    model.eval()\n","    test_loss_list = []\n","\n","    # each data is of batch_size samples\n","    print('Test set loss: [', end='')\n","    for subject in range(9):\n","      test_loss = 0\n","      for i, (data, _) in enumerate(test_loader[str(subject+1)]):\n","          data = data.type(torch.FloatTensor)\n","          if use_cuda and torch.cuda.is_available():\n","              # make sure this lives on the GPU\n","              data = data.cuda()\n","\n","          # we're only going to infer, so no autograd at all required: volatile=True\n","          data = Variable(data, volatile=True)\n","          recon_batch, mu, logvar = model(data)\n","          test_loss += vae_loss_function(recon_batch, data, mu, logvar).data[0]\n","          if i == 0:\n","            n = min(data.size(0), 8)\n","            # for the first 128 batch of the epoch, show the first 8 input digits\n","            # with right below them the reconstructed output digits\n","            comparison = torch.cat([data[:n],\n","                                    recon_batch.view(1, 1, 22, 1000)[:n]])\n","\n","      test_loss /= len(test_loader[str(subject+1)].dataset)\n","      print('{:.4f}, '.format(test_loss), end='')\n","    print(']')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D8QgSQaYUKhK","colab_type":"text"},"cell_type":"markdown","source":["###Train the VAE"]},{"metadata":{"id":"ShrhXTxgUB3x","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":58},{"item_id":117},{"item_id":175},{"item_id":232},{"item_id":275},{"item_id":278}],"base_uri":"https://localhost:8080/","height":3537},"outputId":"4e57a373-a06e-4de5-a698-d8f92705678f","executionInfo":{"status":"ok","timestamp":1521080546134,"user_tz":420,"elapsed":165508,"user":{"displayName":"Nathan Wong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"106327691903472177650"}}},"cell_type":"code","source":["for epoch in range(1, 101):\n","    vae_train(epoch)\n","    vae_test(epoch)"],"execution_count":77,"outputs":[{"output_type":"stream","text":["====> Epoch: 1 Average loss: 15323.9618\n","Test set loss: [15246.0625, 15246.5665, 15245.6041, 15243.8028, 15247.0920, 15241.9612, 15244.7264, 15249.6363, 15245.2642, ]\n","====> Epoch: 2 Average loss: 15245.2052\n","Test set loss: [15245.2591, 15245.9887, 15244.8032, 15242.9437, 15246.3472, 15240.8340, 15244.0160, 15249.2621, 15244.5770, ]\n","====> Epoch: 3 Average loss: 15245.0519\n","Test set loss: [15245.2963, 15246.1561, 15245.0465, 15242.8012, 15246.8002, 15240.7036, 15243.9282, 15249.4593, 15244.5995, ]\n","====> Epoch: 4 Average loss: 15245.3874\n","Test set loss: [15245.0232, 15246.1940, 15245.1352, 15242.9194, 15246.7922, 15240.0490, 15243.7722, 15249.3569, 15244.3540, ]\n","====> Epoch: 5 Average loss: 15245.9726\n","Test set loss: [15245.1778, 15246.5016, 15245.3593, 15243.3317, 15246.9329, 15240.6438, 15243.8772, 15249.0929, 15244.6103, ]\n","====> Epoch: 6 Average loss: 15244.8590\n","Test set loss: [15243.4200, 15245.4454, 15244.4054, 15241.9279, 15245.4810, 15238.7581, 15242.8403, 15247.4583, 15243.4116, ]\n","====> Epoch: 7 Average loss: 15246.3937\n","Test set loss: [15243.8757, 15246.0072, 15244.9666, 15242.4839, 15246.0259, 15239.5107, 15243.1299, 15247.8643, 15243.8399, ]\n","====> Epoch: 8 Average loss: 15243.5606\n","Test set loss: [15244.0751, 15246.5672, 15245.7731, 15243.5717, 15245.7547, 15240.0951, 15243.9954, 15246.7393, 15243.8383, ]\n","====> Epoch: 9 Average loss: 15246.6333\n","Test set loss: [15242.0986, 15245.3379, 15244.2947, 15241.5816, 15244.7648, 15237.8428, 15242.2368, 15246.4615, 15242.2430, ]\n","====> Epoch: 10 Average loss: 15244.2545\n","Test set loss: [15241.7833, 15246.4044, 15245.3197, 15242.7256, 15245.1064, 15238.1901, 15242.6535, 15246.8638, 15241.7519, ]\n","====> Epoch: 11 Average loss: 15243.1040\n","Test set loss: [15241.0837, 15245.7177, 15244.6152, 15241.3877, 15244.5269, 15238.0316, 15241.9072, 15246.2862, 15240.3592, ]\n","====> Epoch: 12 Average loss: 15243.7397\n","Test set loss: [15242.6877, 15247.7024, 15246.8567, 15244.5635, 15246.6764, 15239.8684, 15244.8835, 15247.0349, 15242.8686, ]\n","====> Epoch: 13 Average loss: 15241.6417\n","Test set loss: [15236.2114, 15243.5282, 15242.2682, 15239.4768, 15241.7528, 15234.3054, 15239.9542, 15242.8030, 15237.7858, ]\n","====> Epoch: 14 Average loss: 15240.0479\n","Test set loss: [15242.2829, 15249.2484, 15247.5982, 15245.9060, 15246.4075, 15240.1874, 15245.9294, 15247.1244, 15242.8127, ]\n","====> Epoch: 15 Average loss: 15240.3261\n","Test set loss: [15235.5946, 15244.2785, 15242.4126, 15239.7622, 15241.7136, 15233.4951, 15240.0517, 15243.1386, 15238.2755, ]\n","====> Epoch: 16 Average loss: 15238.3472\n","Test set loss: [15232.1934, 15242.4002, 15240.6199, 15237.0287, 15238.5142, 15230.3577, 15237.2365, 15239.9457, 15233.7249, ]\n","====> Epoch: 17 Average loss: 15237.0262\n","Test set loss: [15232.8009, 15243.6834, 15241.8745, 15239.1345, 15240.6080, 15231.4799, 15239.6341, 15240.5798, 15235.2008, ]\n","====> Epoch: 18 Average loss: 15235.8533\n","Test set loss: [15229.6048, 15241.1746, 15239.2347, 15235.6716, 15237.2618, 15228.2722, 15236.4854, 15237.8880, 15232.4772, ]\n","====> Epoch: 19 Average loss: 15234.9425\n","Test set loss: [15228.1160, 15240.8384, 15238.1771, 15235.2530, 15235.7514, 15226.3530, 15235.9218, 15237.0126, 15231.3868, ]\n","====> Epoch: 20 Average loss: 15233.4544\n","Test set loss: [15228.9837, 15242.1399, 15239.5294, 15235.9237, 15236.3283, 15226.9529, 15236.3303, "],"name":"stdout"},{"output_type":"stream","text":["15237.8050, 15231.1674, ]\n","====> Epoch: 21 Average loss: 15234.1996\n","Test set loss: [15228.4227, 15241.5238, 15239.2915, 15236.0789, 15236.1554, 15226.7547, 15236.1439, 15237.4322, 15231.5660, ]\n","====> Epoch: 22 Average loss: 15231.7785\n","Test set loss: [15225.7881, 15239.4371, 15236.9687, 15233.7883, 15233.7872, 15224.1408, 15233.8068, 15234.8733, 15228.0485, ]\n","====> Epoch: 23 Average loss: 15231.5321\n","Test set loss: [15226.4624, 15240.7956, 15238.2801, 15235.0404, 15235.7076, 15224.7097, 15235.4093, 15235.4653, 15229.3145, ]\n","====> Epoch: 24 Average loss: 15230.5403\n","Test set loss: [15223.6538, 15238.5712, 15236.3805, 15233.1211, 15232.7090, 15222.4164, 15233.1111, 15233.1070, 15226.9466, ]\n","====> Epoch: 25 Average loss: 15229.5935\n","Test set loss: [15223.1361, 15238.5234, 15236.0137, 15232.8103, 15232.2336, 15221.2180, 15232.6222, 15232.5611, 15225.8651, ]\n","====> Epoch: 26 Average loss: 15230.0917\n","Test set loss: [15225.8488, 15240.5160, 15238.4929, 15234.6490, 15234.0769, 15223.7327, 15234.2369, 15234.8552, 15227.7921, ]\n","====> Epoch: 27 Average loss: 15229.3159\n","Test set loss: [15225.1515, 15241.1304, 15238.7129, 15234.8794, 15234.2793, 15224.1719, 15234.3111, 15234.8172, 15226.5779, ]\n","====> Epoch: 28 Average loss: 15228.7822\n","Test set loss: [15223.4442, 15239.2484, 15237.2362, 15233.7834, 15233.3019, 15222.1274, 15233.7300, 15233.8287, 15227.0821, ]\n","====> Epoch: 29 Average loss: 15226.7445\n","Test set loss: [15220.4432, 15237.8445, 15235.3295, 15231.9541, 15230.9501, 15219.7371, 15231.4930, 15230.8425, 15223.6186, ]\n","====> Epoch: 30 Average loss: 15225.8231\n","Test set loss: [15221.7338, 15238.9388, 15235.9686, 15233.1269, 15231.7904, 15220.4887, 15232.3999, 15231.7128, 15223.9961, ]\n","====> Epoch: 31 Average loss: 15225.8643\n","Test set loss: [15220.3390, 15238.5012, 15235.5697, 15232.2024, 15231.2450, 15219.6109, 15232.1644, 15231.0494, 15223.6207, ]\n","====> Epoch: 32 Average loss: 15225.7036\n","Test set loss: [15219.2362, 15236.9964, 15234.0379, 15230.9282, 15229.7071, 15218.4635, 15229.8350, 15230.0284, 15222.0565, ]\n","====> Epoch: 33 Average loss: 15223.8861\n","Test set loss: [15220.0914, 15238.0990, 15235.7259, 15232.4302, 15230.7771, 15218.9748, 15231.5612, 15230.7584, 15222.6052, ]\n","====> Epoch: 34 Average loss: 15223.7859\n","Test set loss: [15219.7935, 15237.3322, 15234.3251, 15231.5394, 15230.2449, 15218.1032, 15230.5629, 15229.9867, 15221.7224, ]\n","====> Epoch: 35 Average loss: 15222.3860\n","Test set loss: [15219.8146, 15238.7224, 15235.0888, 15232.4828, 15231.1151, 15219.0263, 15231.5343, 15230.6900, 15222.2662, ]\n","====> Epoch: 36 Average loss: 15223.2726\n","Test set loss: [15218.7547, 15237.1908, 15234.1542, 15231.1187, 15230.0510, 15217.8330, 15230.2682, 15229.1805, 15221.3040, ]\n","====> Epoch: 37 Average loss: 15221.5574\n","Test set loss: [15217.8800, 15236.8259, 15233.5782, 15230.7915, 15229.2590, 15216.6805, 15229.7429, 15228.3461, 15220.2593, ]\n","====> Epoch: 38 Average loss: 15221.2238\n","Test set loss: [15218.5734, 15237.0052, 15233.9648, 15231.6732, 15229.9859, 15216.9483, 15230.1784, 15228.8105, 15220.8521, ]\n","====> Epoch: 39 Average loss: 15220.4800\n","Test set loss: [15217.4125, 15236.1579, 15233.5912, 15230.5500, 15229.3497, 15216.5906, 15229.4755, 15227.8331, 15219.7548, ]\n","====> Epoch: 40 Average loss: 15219.9807\n","Test set loss: [15216.8239, 15235.8871, 15233.0974, "],"name":"stdout"},{"output_type":"stream","text":["15230.3256, 15229.1138, 15216.3965, 15229.1830, 15227.2452, 15219.0586, ]\n","====> Epoch: 41 Average loss: 15219.4284\n","Test set loss: [15218.4504, 15236.8518, 15233.7760, 15231.4932, 15229.9634, 15217.1933, 15229.7106, 15227.7806, 15219.1211, ]\n","====> Epoch: 42 Average loss: 15219.9529\n","Test set loss: [15217.2425, 15235.5725, 15232.7132, 15230.1435, 15229.0638, 15216.3499, 15228.6195, 15226.5144, 15218.8439, ]\n","====> Epoch: 43 Average loss: 15218.6139\n","Test set loss: [15219.4765, 15236.9672, 15234.2776, 15231.9604, 15231.1030, 15217.9172, 15230.6413, 15228.1709, 15221.0479, ]\n","====> Epoch: 44 Average loss: 15218.0459\n","Test set loss: [15219.2208, 15237.7365, 15234.7432, 15232.9139, 15231.0706, 15218.2641, 15231.2882, 15228.9907, 15221.7409, ]\n","====> Epoch: 45 Average loss: 15218.0382\n","Test set loss: [15217.1530, 15235.7906, 15232.6919, 15230.3914, 15229.1043, 15216.4219, 15229.1680, 15226.2641, 15219.4115, ]\n","====> Epoch: 46 Average loss: 15217.1424\n","Test set loss: [15218.3777, 15235.9910, 15232.9770, 15230.6502, 15229.5422, 15216.9431, 15229.1986, 15226.7859, 15219.0501, ]\n","====> Epoch: 47 Average loss: 15216.4281\n","Test set loss: [15218.6807, 15236.3806, 15233.6605, 15231.6314, 15229.9694, 15217.2889, 15230.1936, 15227.5920, 15220.5307, ]\n","====> Epoch: 48 Average loss: 15216.1258\n","Test set loss: [15218.6640, 15236.9730, 15233.8975, 15231.5754, 15230.6157, 15217.8575, 15230.2602, 15227.9695, 15220.3959, ]\n","====> Epoch: 49 Average loss: 15215.7065\n","Test set loss: [15219.2176, 15236.4091, 15233.2552, 15231.3654, 15230.2781, 15217.2821, 15230.1092, 15227.3662, 15221.4620, ]\n","====> Epoch: 50 Average loss: 15215.2051\n","Test set loss: [15218.6722, 15235.9782, 15232.9266, 15230.6104, 15229.1657, 15217.1733, 15229.0884, 15226.5492, 15219.4941, ]\n","====> Epoch: 51 Average loss: 15214.6455\n","Test set loss: [15218.8726, 15236.5368, 15233.4994, 15231.2039, 15230.4549, 15218.2238, 15229.7957, 15227.7437, 15220.5801, ]\n","====> Epoch: 52 Average loss: 15214.8171\n","Test set loss: [15219.3066, 15236.8335, 15234.2485, 15231.4756, 15230.3706, 15218.0742, 15230.4357, 15227.7600, 15220.9455, ]\n","====> Epoch: 53 Average loss: 15214.2474\n","Test set loss: [15220.8371, 15235.8871, 15233.2442, 15230.8040, 15231.0556, 15218.8450, 15230.1675, 15227.9352, 15221.7962, ]\n","====> Epoch: 54 Average loss: 15213.6804\n","Test set loss: [15219.9554, 15236.3306, 15233.3353, 15231.3632, 15230.0585, 15217.8997, 15229.6857, 15228.0042, 15221.6403, ]\n","====> Epoch: 55 Average loss: 15214.6013\n","Test set loss: [15220.8421, 15236.2115, 15233.3766, 15231.1541, 15229.7800, 15218.0809, 15229.5551, 15228.5245, 15221.1078, ]\n","====> Epoch: 56 Average loss: 15213.4168\n","Test set loss: [15221.0822, 15236.6499, 15233.8568, 15231.5220, 15230.6489, 15218.7513, 15230.4759, 15228.4526, 15222.4218, ]\n","====> Epoch: 57 Average loss: 15212.8677\n","Test set loss: [15220.5255, 15236.2302, 15232.8803, 15230.5989, 15229.9804, 15217.9328, 15229.3818, 15228.3345, 15221.5262, ]\n","====> Epoch: 58 Average loss: 15212.2736\n","Test set loss: [15223.2177, 15237.9499, 15234.8857, 15232.3918, 15232.7374, 15220.9570, 15231.7096, 15230.6803, 15225.0150, ]\n","====> Epoch: 59 Average loss: 15212.4677\n","Test set loss: [15221.4784, 15236.3872, 15233.2965, 15230.7748, 15230.6319, 15218.7738, 15230.0679, 15228.7068, 15222.7723, ]\n"],"name":"stdout"},{"output_type":"stream","text":["====> Epoch: 60 Average loss: 15211.6371\n","Test set loss: [15223.0204, 15236.8460, 15234.6872, 15231.9538, 15231.0245, 15219.5063, 15230.8494, 15229.5985, 15223.9682, ]\n","====> Epoch: 61 Average loss: 15212.0943\n","Test set loss: [15225.1959, 15238.7543, 15235.7283, 15233.9635, 15234.7318, 15224.6030, 15233.4788, 15232.5044, 15230.5709, ]\n","====> Epoch: 62 Average loss: 15212.4113\n","Test set loss: [15222.0728, 15236.1681, 15234.1411, 15231.1590, 15231.0109, 15219.0982, 15230.2535, 15229.8307, 15225.2750, ]\n","====> Epoch: 63 Average loss: 15210.7076\n","Test set loss: [15224.7698, 15237.3972, 15234.9279, 15232.2659, 15232.7799, 15221.6920, 15231.8646, 15231.6960, 15227.5169, ]\n","====> Epoch: 64 Average loss: 15211.6868\n","Test set loss: [15222.0743, 15236.2795, 15233.8402, 15231.1285, 15230.7012, 15220.5966, 15230.1470, 15230.3823, 15225.5326, ]\n","====> Epoch: 65 Average loss: 15211.0141\n","Test set loss: [15223.6656, 15236.3038, 15233.9380, 15231.8360, 15231.8214, 15221.2303, 15230.6907, 15230.8489, 15225.4394, ]\n","====> Epoch: 66 Average loss: 15209.4306\n","Test set loss: [15223.7872, 15237.7289, 15234.7685, 15232.5779, 15231.7069, 15221.0615, 15231.2393, 15231.0305, 15226.6186, ]\n","====> Epoch: 67 Average loss: 15210.0240\n","Test set loss: [15224.3590, 15236.7232, 15233.9029, 15231.6990, 15231.2762, 15219.9489, 15230.7660, 15230.6580, 15226.8296, ]\n","====> Epoch: 68 Average loss: 15209.2682\n","Test set loss: [15225.0364, 15236.8948, 15234.8380, 15232.1517, 15232.3349, 15221.0922, 15231.4887, 15232.3755, 15227.3427, ]\n","====> Epoch: 69 Average loss: 15208.5933\n","Test set loss: [15224.1524, 15236.7084, 15234.0599, 15231.6233, 15231.8164, 15222.0145, 15230.6009, 15231.8096, 15229.0290, ]\n","====> Epoch: 70 Average loss: 15208.2891\n","Test set loss: [15227.7763, 15237.6344, 15234.9522, 15232.6154, 15233.5133, 15223.1028, 15232.2716, 15234.1929, 15229.4399, ]\n","====> Epoch: 71 Average loss: 15209.1862\n","Test set loss: [15226.8297, 15236.5471, 15234.5510, 15231.9182, 15232.5089, 15222.3480, 15231.6416, 15231.9028, 15229.9757, ]\n","====> Epoch: 72 Average loss: 15208.3570\n","Test set loss: [15229.0832, 15237.6150, 15235.5921, 15232.6679, 15234.4152, 15225.5620, 15232.9863, 15233.8849, 15231.6221, ]\n","====> Epoch: 73 Average loss: 15207.6252\n","Test set loss: [15225.6661, 15237.0532, 15234.2643, 15231.3846, 15231.9273, 15223.7769, 15231.0934, 15231.3607, 15228.1346, ]\n","====> Epoch: 74 Average loss: 15207.7527\n","Test set loss: [15225.5633, 15236.7854, 15234.7103, 15231.7995, 15232.2392, 15223.2730, 15231.1994, 15231.4168, 15229.2347, ]\n","====> Epoch: 75 Average loss: 15206.5269\n","Test set loss: [15226.6235, 15236.8140, 15234.4513, 15231.5330, 15232.2811, 15223.6356, 15231.0253, 15234.5821, 15229.2279, ]\n","====> Epoch: 76 Average loss: 15205.6125\n","Test set loss: [15226.4085, 15237.3157, 15234.6008, 15231.5165, 15232.4665, 15225.0662, 15231.4366, 15233.5669, 15229.4000, ]\n","====> Epoch: 77 Average loss: 15206.4693\n","Test set loss: [15230.7310, 15237.5105, 15235.9830, 15232.4412, 15234.5570, 15225.2038, 15233.5652, 15235.1445, 15232.8362, ]\n","====> Epoch: 78 Average loss: 15207.2376\n","Test set loss: [15229.5129, 15238.5240, 15236.3020, 15233.3892, 15235.3197, 15227.6088, 15233.4436, 15236.0616, 15232.3031, ]\n","====> Epoch: 79 Average loss: 15206.6505\n","Test set loss: [15229.6915, 15237.6470, 15234.7050, 15232.2401, 15234.1323, 15226.6961, 15231.9693, "],"name":"stdout"},{"output_type":"stream","text":["15237.4718, 15233.0496, ]\n","====> Epoch: 80 Average loss: 15206.7776\n","Test set loss: [15231.8291, 15238.2588, 15235.6147, 15232.4346, 15234.1515, 15225.5176, 15233.3468, 15235.2985, 15231.2961, ]\n","====> Epoch: 81 Average loss: 15206.1695\n","Test set loss: [15232.9491, 15237.7744, 15236.7722, 15234.1323, 15235.5728, 15226.7232, 15234.4468, 15236.2996, 15237.4903, ]\n","====> Epoch: 82 Average loss: 15204.4489\n","Test set loss: [15231.0120, 15238.5600, 15236.5641, 15233.0386, 15234.2761, 15225.6355, 15233.0411, 15235.1001, 15230.1587, ]\n","====> Epoch: 83 Average loss: 15206.0252\n","Test set loss: [15229.2099, 15238.5197, 15236.0512, 15233.3301, 15234.1230, 15226.7411, 15233.6210, 15235.4699, 15234.5603, ]\n","====> Epoch: 84 Average loss: 15205.6204\n","Test set loss: [15229.0604, 15236.9098, 15235.0285, 15232.0501, 15233.5148, 15226.1821, 15232.3935, 15235.6032, 15234.0652, ]\n","====> Epoch: 85 Average loss: 15203.7980\n","Test set loss: [15228.5157, 15236.4178, 15234.7979, 15230.9892, 15231.7916, 15224.6293, 15230.9701, 15234.7190, 15229.2059, ]\n","====> Epoch: 86 Average loss: 15204.1007\n","Test set loss: [15232.1539, 15238.1758, 15236.4389, 15232.3670, 15234.8210, 15227.8987, 15233.5996, 15236.4950, 15233.3314, ]\n","====> Epoch: 87 Average loss: 15203.6812\n","Test set loss: [15230.4311, 15236.9321, 15235.1928, 15232.2406, 15233.5308, 15224.9881, 15233.2040, 15236.8177, 15236.5070, ]\n","====> Epoch: 88 Average loss: 15204.0869\n","Test set loss: [15231.8404, 15237.4846, 15236.5305, 15232.7232, 15234.8666, 15227.5029, 15233.8074, 15236.8522, 15234.7907, ]\n","====> Epoch: 89 Average loss: 15205.8034\n","Test set loss: [15232.8809, 15238.9130, 15237.1733, 15233.7691, 15236.6709, 15226.8722, 15234.3054, 15237.1287, 15234.1047, ]\n","====> Epoch: 90 Average loss: 15204.9787\n","Test set loss: [15232.5049, 15239.2047, 15238.3288, 15234.5564, 15237.1158, 15229.4586, 15234.7350, 15242.3634, 15239.3436, ]\n","====> Epoch: 91 Average loss: 15204.1991\n","Test set loss: [15231.8752, 15238.5558, 15236.4113, 15233.1791, 15235.1564, 15227.3409, 15233.5582, 15238.5006, 15235.1058, ]\n","====> Epoch: 92 Average loss: 15204.5604\n","Test set loss: [15228.3273, 15236.9654, 15235.1469, 15232.3156, 15232.0359, 15224.0448, 15232.0920, 15234.9929, 15230.1419, ]\n","====> Epoch: 93 Average loss: 15203.6968\n","Test set loss: [15232.0224, 15238.2848, 15236.5128, 15233.6199, 15234.8937, 15227.6506, 15235.2949, 15236.3366, 15235.8805, ]\n","====> Epoch: 94 Average loss: 15203.2482\n","Test set loss: [15232.0694, 15237.1121, 15235.8336, 15233.5606, 15235.1124, 15227.2214, 15234.4287, 15237.3736, 15240.4363, ]\n","====> Epoch: 95 Average loss: 15200.9148\n","Test set loss: [15230.5897, 15236.7733, 15235.6235, 15232.3491, 15234.3406, 15227.7554, 15233.6636, 15235.9989, 15238.1104, ]\n","====> Epoch: 96 Average loss: 15201.3366\n","Test set loss: [15233.1049, 15237.6249, 15236.1737, 15232.7176, 15234.6892, 15227.5780, 15233.1744, 15239.7001, 15239.2948, ]\n","====> Epoch: 97 Average loss: 15201.4098\n","Test set loss: [15234.1730, 15237.9979, 15236.4226, 15233.3569, 15236.5292, 15233.0452, 15235.6957, 15240.9726, 15242.4499, ]\n","====> Epoch: 98 Average loss: 15201.9401\n","Test set loss: [15233.9220, 15237.6717, 15236.6554, 15234.0871, 15234.6248, 15229.6152, 15234.6040, 15240.3918, 15244.0299, ]\n","====> Epoch: 99 Average loss: 15201.3699\n","Test set loss: [15232.0641, 15236.8990, 15235.1882, "],"name":"stdout"},{"output_type":"stream","text":["15231.1860, 15232.8546, 15228.2245, 15231.5245, 15236.5215, 15234.1919, ]\n","====> Epoch: 100 Average loss: 15200.6278\n","Test set loss: [15236.2091, 15238.5571, 15238.6511, 15233.9654, 15236.3086, 15233.1301, 15235.7608, 15244.1993, 15246.0118, ]\n"],"name":"stdout"}]},{"metadata":{"id":"in2BgFpNZX-v","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":28}],"base_uri":"https://localhost:8080/","height":809},"outputId":"30ce034f-e4a5-4c2f-eed0-0979c35e3d0e"},"cell_type":"code","source":["training_acc_arr = np.empty(num_epochs)\n","testing_acc_arr = np.empty((9, num_epochs))\n","\n","for epoch in range(num_epochs):\n","  \n","  net.train()\n","  \n","  for i, (signals, labels) in enumerate(train_loader):\n","    \n","    signals = signals.type(torch.FloatTensor)\n","    signals = Variable(signals)\n","    labels = labels.type(torch.LongTensor)\n","    labels = Variable(torch.squeeze(labels))\n","    \n","    if use_cuda and torch.cuda.is_available():\n","      signals = signals.cuda()\n","      labels = labels.cuda()\n","    \n","    if epoch > 80:\n","      signals, _, _ = model(signals)\n","      signals = signals.view(-1, 1, 22, 1000)\n","    \n","    optimizer.zero_grad()\n","    outputs = net(signals)\n","    \n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    if (i+1) % 17 == 0:\n","      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n","            % (epoch+1, num_epochs, i+1, len(EEGset.train_dataset)//batch_size, \n","               loss.data[0]))\n","  \n","  net.eval()\n","  \n","  # Training accuracy\n","  total = 0\n","  correct = 0\n","  for signals, labels in train_loader:\n","    signals = signals.type(torch.FloatTensor)\n","    signals = Variable(signals)\n","    labels = torch.squeeze(labels.type(torch.LongTensor))\n","    if use_cuda and torch.cuda.is_available():\n","      signals = signals.cuda()\n","      labels = labels.cuda()\n","    outputs = net(signals)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum()\n","  training_acc_arr[epoch] = (correct/total)\n","  print ('Training Accuracy: %.5f' % training_acc_arr[epoch])\n","  \n","  \n","  # Testing accuracy\n","  for subject in range(9):\n","    total = 0\n","    correct = 0\n","    for signals, labels in test_loader[str(subject+1)]:\n","      signals = signals.type(torch.FloatTensor)\n","      signals = Variable(signals)\n","      labels = torch.squeeze(labels.type(torch.LongTensor))\n","      if use_cuda and torch.cuda.is_available():\n","        signals = signals.cuda()\n","        labels = labels.cuda()\n","      outputs = net(signals)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum()\n","    testing_acc_arr[subject, epoch] = (correct/total)\n","  print ('Testing Accuracy: ' + str(testing_acc_arr[:, epoch]))\n","  print ('Testing Accuracy Average: %.5f' % np.average(testing_acc_arr[:, epoch]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch [1/200], Step [17/68], Loss: 1.3801\n","Epoch [1/200], Step [34/68], Loss: 1.3819\n","Epoch [1/200], Step [51/68], Loss: 1.3932\n","Epoch [1/200], Step [68/68], Loss: 1.3790\n","Training Accuracy: 0.25522\n","Testing Accuracy: [0.22 0.2  0.22 0.24 0.2  0.28 0.22 0.22 0.24]\n","Testing Accuracy Average: 0.22667\n","Epoch [2/200], Step [17/68], Loss: 1.3914\n","Epoch [2/200], Step [34/68], Loss: 1.3771\n","Epoch [2/200], Step [51/68], Loss: 1.3900\n","Epoch [2/200], Step [68/68], Loss: 1.4102\n","Training Accuracy: 0.25522\n","Testing Accuracy: [0.22 0.2  0.22 0.24 0.2  0.28 0.22 0.22 0.24]\n","Testing Accuracy Average: 0.22667\n","Epoch [3/200], Step [17/68], Loss: 1.3862\n","Epoch [3/200], Step [34/68], Loss: 1.3920\n","Epoch [3/200], Step [51/68], Loss: 1.3954\n","Epoch [3/200], Step [68/68], Loss: 1.3966\n","Training Accuracy: 0.25522\n","Testing Accuracy: [0.22 0.2  0.22 0.24 0.2  0.28 0.22 0.22 0.24]\n","Testing Accuracy Average: 0.22667\n","Epoch [4/200], Step [17/68], Loss: 1.3919\n","Epoch [4/200], Step [34/68], Loss: 1.3885\n","Epoch [4/200], Step [51/68], Loss: 1.3872\n","Epoch [4/200], Step [68/68], Loss: 1.3819\n","Training Accuracy: 0.25522\n","Testing Accuracy: [0.22 0.2  0.22 0.24 0.2  0.28 0.22 0.22 0.24]\n","Testing Accuracy Average: 0.22667\n","Epoch [5/200], Step [17/68], Loss: 1.3809\n","Epoch [5/200], Step [34/68], Loss: 1.3853\n","Epoch [5/200], Step [51/68], Loss: 1.3864\n","Epoch [5/200], Step [68/68], Loss: 1.3721\n","Training Accuracy: 0.25522\n","Testing Accuracy: [0.22 0.2  0.22 0.24 0.2  0.28 0.22 0.22 0.24]\n","Testing Accuracy Average: 0.22667\n","Epoch [6/200], Step [17/68], Loss: 1.3809\n","Epoch [6/200], Step [34/68], Loss: 1.3941\n","Epoch [6/200], Step [51/68], Loss: 1.3900\n","Epoch [6/200], Step [68/68], Loss: 1.3881\n","Training Accuracy: 0.25522\n","Testing Accuracy: [0.22 0.2  0.22 0.24 0.2  0.28 0.22 0.22 0.24]\n","Testing Accuracy Average: 0.22667\n","Epoch [7/200], Step [17/68], Loss: 1.3965\n","Epoch [7/200], Step [34/68], Loss: 1.3899\n","Epoch [7/200], Step [51/68], Loss: 1.3893\n"],"name":"stdout"}]},{"metadata":{"id":"k2eMKn9WaiTz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}